{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3e67f200",
   "metadata": {},
   "source": [
    "# How to call functions with chat models\n",
    "\n",
    "This notebook covers how to use the Chat Completions API in combination with external functions to extend the capabilities of GPT models.\n",
    "\n",
    "## How to use functions\n",
    "\n",
    "`functions` is an optional parameter in the ChatCompletion API which can be used to provide function specifications. The purpose of this is to enable models to generate outputs which adhere to function input schemas. Note that the API will not actually execute any function calls. It is up to developers to execute function calls using model outputs.\n",
    "\n",
    "If the `functions` parameter is provided then by default the model will decide when it is appropriate to use one of the functions. The API can also be forced to use a specific function by setting the `function_call` parameter to `{\"name\": \"<insert-function-name>\"}`. If a function is used, the output will contain `\"finish_reason\": \"function_call\"` in the response, as well as a `function_call` object that has the name of the function and the generated function arguments.\n",
    "\n",
    "Functions are specified with the following fields:\n",
    "\n",
    "- **Name:** The name of the function.\n",
    "- **Description:** A description of what the function does. The model will use this to decide when to call the function.\n",
    "- **Parameters:** The parameters object contains all of the input fields the function requires. These inputs can be of the following types: String, Number, Boolean, Object, Null, AnyOf. Refer to the [API reference docs](https://platform.openai.com/docs/api-reference/chat) for details.\n",
    "- **Required:** Which of the parameters are required to make a query. The rest will be treated as optional.\n",
    "\n",
    "You can chain function calls by executing the function and passing the output of the function execution directly back to the assistant. This can lead to _infinite loop_ behaviour where the model continues calling functions indefinitely, however guardrails can be put in place to prevent this.\n",
    "\n",
    "## Walkthrough\n",
    "\n",
    "This cookbook takes you through the following workflow:\n",
    "\n",
    "- **Basic concepts:** Creating an example function and getting the API to use it if appropriate.\n",
    "- **Integrating API calls with function execution:** Creating an agent that uses API calls to generate function arguments and then executes the function.\n",
    "- **Using multiple functions:** Allowing multiple functions to be called in sequence before responding to the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dab872c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import ast\n",
    "import concurrent\n",
    "from csv import writer\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "import requests\n",
    "from scipy import spatial\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69ee6a93",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "First let's define a few utilities for making calls to the Chat Completions API and for maintaining and keeping track of the conversation state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "745ceec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentai.api import chat_complete\n",
    "from agentai.conversation import Conversation\n",
    "from agentai.function_parser import function_info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29d4e02b",
   "metadata": {},
   "source": [
    "## Basic concepts\n",
    "\n",
    "Next we'll create a specification for a function called ```get_current_weather```. Later we'll pass this function specification to the API in order to generate function arguments that adhere to the specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2e25069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class TemperatureUnit(Enum):\n",
    "    celsius = \"celsius\"\n",
    "    fahrenheit = \"fahrenheit\"\n",
    "\n",
    "@function_info\n",
    "def get_current_weather(location: str, format: TemperatureUnit) -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather\n",
    "\n",
    "    Args:\n",
    "        location (str): The city and state, e.g. San Francisco, CA\n",
    "        format (str): The temperature unit to use. Infer this from the users location.\n",
    "\n",
    "    Returns:\n",
    "        str: The current weather\n",
    "    \"\"\"\n",
    "    # Your function implementation goes here.\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "\n",
    "registered_functions = [get_current_weather]\n",
    "functions = [json.loads(func.json_info) for func in registered_functions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "131d9a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No response from assistant, trying again\n",
      "No response from assistant, trying again\n",
      "No response from assistant, trying again\n",
      "No response from assistant, trying again\n",
      "No response from assistant, trying again\n",
      "No response from assistant, trying again\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'Which city and state would you like to know the weather for?'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = Conversation()\n",
    "conversation.add_message(\"user\", \"what is the weather like today?\")\n",
    "\n",
    "message = {'role': 'assistant', 'content': None}\n",
    "chat_response = chat_complete(conversation.conversation_history, functions=functions, model=GPT_MODEL)\n",
    "message = chat_response.json()[\"choices\"][0][\"message\"]\n",
    "while message[\"content\"] is None:\n",
    "    print(\"No response from assistant, trying again\")\n",
    "    chat_response = chat_complete(conversation.conversation_history, functions=functions, model=GPT_MODEL)\n",
    "    message = chat_response.json()[\"choices\"][0][\"message\"]\n",
    "conversation.add_message(message[\"role\"], message[\"content\"])\n",
    "message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ee28e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'what is the weather like today?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Which city and state would you like to know the weather for?'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "854b6e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 0,\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': None,\n",
       "  'function_call': {'name': 'get_current_weather',\n",
       "   'arguments': '{\\n  \"location\": \"Bengaluru, India\",\\n  \"format\": \"celsius\"\\n}'}},\n",
       " 'finish_reason': 'function_call'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Once the user provides the required information, the model can generate the function arguments\n",
    "conversation.add_message(\"user\", \"I'm in Bengaluru, India\")\n",
    "chat_response = chat_complete(conversation.conversation_history, functions=functions, model=GPT_MODEL)\n",
    "\n",
    "chat_response.json()[\"choices\"][0]\n",
    "# chat_response.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4482aee",
   "metadata": {},
   "source": [
    "## Integrating API calls with function execution\n",
    "\n",
    "In our next example, we'll demonstrate how to execute functions whose inputs are model-generated, and use this to implement an agent that can answer questions for us about a database. For simplicity we'll use the [Chinook sample database](https://www.sqlitetutorial.net/sqlite-sample-database/).\n",
    "\n",
    "*Note:* SQL generation use cases are high-risk in a production environment - models can be unreliable when generating consistent SQL syntax. A more reliable way to solve this problem may be to build a query generation API that takes the desired columns as input from the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7654fef",
   "metadata": {},
   "source": [
    "### Pull SQL Database Info\n",
    "\n",
    "First let's define some helpful utility functions to extract data from a SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30f6b60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opened database successfully\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"../data/Chinook.db\")\n",
    "print(\"Opened database successfully\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77e6e5ea",
   "metadata": {},
   "source": [
    "Now can use these utility functions to extract a representation of the database schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c0104cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentai.sqlite_utils import DBUtils\n",
    "database_schema_dict = DBUtils(conn).get_database_info()\n",
    "database_schema_string = \"\\n\".join(\n",
    "    [f\"Table: {table['table_name']}\\nColumns: {', '.join(table['column_names'])}\" for table in database_schema_dict]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f05b9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: Album\n",
      "Columns: AlbumId, Title, ArtistId\n",
      "Table: Artist\n",
      "Columns: ArtistId, Name\n",
      "Table: Customer\n",
      "Columns: CustomerId, FirstName, LastName, Company, Address, City, State, Country, PostalCode, Phone, Fax, Email, SupportRepId\n",
      "Table: Employee\n",
      "Columns: EmployeeId, LastName, FirstName, Title, ReportsTo, BirthDate, HireDate, Address, City, State, Country, PostalCode, Phone, Fax, Email\n",
      "Table: Genre\n",
      "Columns: GenreId, Name\n",
      "Table: Invoice\n",
      "Columns: InvoiceId, CustomerId, InvoiceDate, BillingAddress, BillingCity, BillingState, BillingCountry, BillingPostalCode, Total\n",
      "Table: InvoiceLine\n",
      "Columns: InvoiceLineId, InvoiceId, TrackId, UnitPrice, Quantity\n",
      "Table: MediaType\n",
      "Columns: MediaTypeId, Name\n",
      "Table: Playlist\n",
      "Columns: PlaylistId, Name\n",
      "Table: PlaylistTrack\n",
      "Columns: PlaylistId, TrackId\n",
      "Table: Track\n",
      "Columns: TrackId, Name, AlbumId, MediaTypeId, GenreId, Composer, Milliseconds, Bytes, UnitPrice\n"
     ]
    }
   ],
   "source": [
    "print(database_schema_string)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae73c9ee",
   "metadata": {},
   "source": [
    "As before, we'll define a function specification for the function we'd like the API to generate arguments for. Notice that we are inserting the database schema into the function specification. This will be important for the model to know about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0258813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@function_info\n",
    "def ask_database(query:str) -> str:\n",
    "    \"\"\"\n",
    "    Use this function to answer user questions about music. Input should be a fully formed SQL query.\n",
    "\n",
    "    Args:\n",
    "        query (str):SQL query extracting info to answer the user's question.\n",
    "                    SQL should be written using this database schema:\n",
    "                    Table: Album\n",
    "                    Columns: AlbumId, Title, ArtistId\n",
    "                    Table: Artist\n",
    "                    Columns: ArtistId, Name\n",
    "                    Table: Customer\n",
    "                    Columns: CustomerId, FirstName, LastName, Company, Address, City, State, Country, PostalCode, Phone, Fax, Email, SupportRepId\n",
    "                    Table: Employee\n",
    "                    Columns: EmployeeId, LastName, FirstName, Title, ReportsTo, BirthDate, HireDate, Address, City, State, Country, PostalCode, Phone, Fax, Email\n",
    "                    Table: Genre\n",
    "                    Columns: GenreId, Name\n",
    "                    Table: Invoice\n",
    "                    Columns: InvoiceId, CustomerId, InvoiceDate, BillingAddress, BillingCity, BillingState, BillingCountry, BillingPostalCode, Total\n",
    "                    Table: InvoiceLine\n",
    "                    Columns: InvoiceLineId, InvoiceId, TrackId, UnitPrice, Quantity\n",
    "                    Table: MediaType\n",
    "                    Columns: MediaTypeId, Name\n",
    "                    Table: Playlist\n",
    "                    Columns: PlaylistId, Name\n",
    "                    Table: PlaylistTrack\n",
    "                    Columns: PlaylistId, TrackId\n",
    "                    Table: Track\n",
    "                    Columns: TrackId, Name, AlbumId, MediaTypeId, GenreId, Composer, Milliseconds, Bytes, UnitPrice\n",
    "                    \n",
    "                    IMPORTANT: Please return a fixed SQL in PLAIN TEXT.\n",
    "                    Your response should consist of ONLY the SQL query.\n",
    "\n",
    "    Returns:\n",
    "        str: _description_\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = conn.execute(query).fetchall()\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"SQL error: {e}\")\n",
    "\n",
    "agentai_functions = [json.loads(func.json_info) for func in [ask_database]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da08c121",
   "metadata": {},
   "source": [
    "### SQL execution\n",
    "\n",
    "Now let's implement the function that the agent will use to query the database. We also need to implement utilities to integrate the calls to the Chat Completions API with the function it is calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5f845a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentai.api import chat_complete_execute_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38c55083",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_system_message = \"\"\"You are ChinookGPT, a helpful assistant who gets answers to user questions from the Chinook Music Database.\n",
    "Provide as many details as possible to your users\n",
    "Begin!\"\"\"\n",
    "\n",
    "sql_conversation = Conversation()\n",
    "sql_conversation.add_message(role=\"system\", content=agent_system_message)\n",
    "sql_conversation.add_message(\"user\", \"Hi, who are the top 5 artists by number of tracks\")\n",
    "assistant_message = chat_complete_execute_fn(sql_conversation, agentai_functions, GPT_MODEL, ask_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28471ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msystem: You are ChinookGPT, a helpful assistant who gets answers to user questions from the Chinook Music Database.\n",
      "Provide as many details as possible to your users\n",
      "Begin!\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[32muser: Hi, who are the top 5 artists by number of tracks\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35mfunction: [('Iron Maiden', 213), ('U2', 135), ('Led Zeppelin', 114), ('Metallica', 112), ('Lost', 92)]\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34massistant: The top 5 artists by the number of tracks are:\n",
      "\n",
      "1. Iron Maiden - 213 tracks\n",
      "2. U2 - 135 tracks\n",
      "3. Led Zeppelin - 114 tracks\n",
      "4. Metallica - 112 tracks\n",
      "5. Lost - 92 tracks\n",
      "\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sql_conversation.display_conversation(detailed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "710481dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_conversation.add_message(\"user\", \"What is the name of the album with the most tracks\")\n",
    "chat_response = chat_complete_execute_fn(sql_conversation, agentai_functions, GPT_MODEL, ask_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "13984dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msystem: You are ChinookGPT, a helpful assistant who gets answers to user questions from the Chinook Music Database.\n",
      "Provide as many details as possible to your users\n",
      "Begin!\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[32muser: Hi, who are the top 5 artists by number of tracks\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35mfunction: [('Iron Maiden', 213), ('U2', 135), ('Led Zeppelin', 114), ('Metallica', 112), ('Lost', 92)]\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34massistant: The top 5 artists by the number of tracks are:\n",
      "\n",
      "1. Iron Maiden - 213 tracks\n",
      "2. U2 - 135 tracks\n",
      "3. Led Zeppelin - 114 tracks\n",
      "4. Metallica - 112 tracks\n",
      "5. Lost - 92 tracks\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[32muser: What is the name of the album with the most tracks\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35mfunction: [('Greatest Hits', 57)]\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34massistant: The album with the most tracks is called \"Greatest Hits\" and it has 57 tracks.\n",
      "\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sql_conversation.display_conversation(detailed=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c282b9e3",
   "metadata": {},
   "source": [
    "## Using Multiple Functions\n",
    "\n",
    "Now let's construct a scenario in which we provide a model with more than one function to call. We'll create an agent that uses data from arXiv to answer questions about academic subjects. It has two new functions at its disposal:\n",
    "- **get_articles**: A function that gets arXiv articles on a subject and summarizes them for the user with links.\n",
    "- **read_article_and_summarize**: This function takes one of the previously searched articles, reads it in its entirety and summarizes the core argument, evidence and conclusions.\n",
    "\n",
    "This will get you comfortable with a multi-function workflow that can choose from multiple services, and where some of the data from the first function is persisted to be used by the second."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2e47962",
   "metadata": {},
   "source": [
    "### arXiv search\n",
    "\n",
    "We'll first set up some utilities that will underpin our two functions.\n",
    "\n",
    "Downloaded papers will be stored in a directory (we use ```./data/papers``` here). We create a file ```arxiv_library.csv``` to store the embeddings and details for downloaded papers to retrieve against using ```summarize_text```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a directory to store downloaded papers\n",
    "data_dir = os.path.join(os.curdir, \"data\", \"papers\")\n",
    "paper_dir_filepath = \"./data/arxiv_library.csv\"\n",
    "\n",
    "# Generate a blank dataframe where we can store downloaded files\n",
    "df = pd.DataFrame(list())\n",
    "df.to_csv(paper_dir_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57217b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def embedding_request(text):\n",
    "    response = openai.Embedding.create(input=text, model=EMBEDDING_MODEL)\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_articles(query, library=paper_dir_filepath, top_k=5):\n",
    "    \"\"\"This function gets the top_k articles based on a user's query, sorted by relevance.\n",
    "    It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize.\n",
    "    \"\"\"\n",
    "    search = arxiv.Search(query=query, max_results=top_k, sort_by=arxiv.SortCriterion.Relevance)\n",
    "    result_list = []\n",
    "    for result in search.results():\n",
    "        result_dict = {}\n",
    "        result_dict.update({\"title\": result.title})\n",
    "        result_dict.update({\"summary\": result.summary})\n",
    "\n",
    "        # Taking the first url provided\n",
    "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
    "        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
    "        result_list.append(result_dict)\n",
    "\n",
    "        # Store references in library file\n",
    "        response = embedding_request(text=result.title)\n",
    "        file_reference = [\n",
    "            result.title,\n",
    "            result.download_pdf(data_dir),\n",
    "            response[\"data\"][0][\"embedding\"],\n",
    "        ]\n",
    "\n",
    "        # Write to file\n",
    "        with open(library, \"a\") as f_object:\n",
    "            writer_object = writer(f_object)\n",
    "            writer_object.writerow(file_reference)\n",
    "            f_object.close()\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda02bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the search is working\n",
    "result_output = get_articles(\"ppo reinforcement learning\")\n",
    "result_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11675627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100,\n",
    ") -> list[str]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = embedding_request(query)\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"filepath\"], relatedness_fn(query_embedding, row[\"embedding\"])) for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(filepath):\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(filepath)\n",
    "    pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return pdf_text\n",
    "\n",
    "\n",
    "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "def create_chunks(text, n, tokenizer):\n",
    "    \"\"\"Returns successive n-sized chunks from provided text.\"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def extract_chunk(content, template_prompt):\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarize chunk of text\"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=GPT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def summarize_text(query):\n",
    "    \"\"\"This function does the following:\n",
    "    - Reads in the arxiv_library.csv file in including the embeddings\n",
    "    - Finds the closest file to the user's query\n",
    "    - Scrapes the text out of the file and chunks it\n",
    "    - Summarizes each chunk in parallel\n",
    "    - Does one final summary and returns this to the user\"\"\"\n",
    "\n",
    "    # A prompt to dictate how the recursive summarizations should approach the input paper\n",
    "    summary_prompt = (\n",
    "        \"\"\"Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\"\"\"\n",
    "    )\n",
    "\n",
    "    # If the library is empty (no searches have been performed yet), we perform one and download the results\n",
    "    library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    if len(library_df) == 0:\n",
    "        print(\"No papers searched yet, downloading first.\")\n",
    "        get_articles(query)\n",
    "        print(\"Papers downloaded, continuing\")\n",
    "        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    library_df.columns = [\"title\", \"filepath\", \"embedding\"]\n",
    "    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
    "    strings = strings_ranked_by_relatedness(query, library_df, top_n=1)\n",
    "    print(\"Chunking text from paper\")\n",
    "    pdf_text = read_pdf(strings[0])\n",
    "\n",
    "    # Initialise tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    results = \"\"\n",
    "\n",
    "    # Chunk up the document into 1500 token chunks\n",
    "    chunks = create_chunks(pdf_text, 1500, tokenizer)\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "    print(\"Summarizing each chunk of text\")\n",
    "\n",
    "    # Parallel process the summaries\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(text_chunks)) as executor:\n",
    "        futures = [executor.submit(extract_chunk, chunk, summary_prompt) for chunk in text_chunks]\n",
    "        with tqdm(total=len(text_chunks)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            results += data\n",
    "\n",
    "    # Final summary\n",
    "    print(\"Summarizing into overall summary\")\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Write a summary collated from this collection of key points extracted from an academic paper.\n",
    "                        The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "                        User query: {query}\n",
    "                        The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "                        Key points:\\n{results}\\nSummary:\\n\"\"\",\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b94d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the summarize_text function works\n",
    "chat_test_response = summarize_text(\"PPO reinforcement learning sequence generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c715f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chat_test_response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93a9f651",
   "metadata": {},
   "source": [
    "### Configure Agent\n",
    "\n",
    "We'll now create 2 function specifications for functions that provide access to the arXiv data. We'll also create some more utilities to integrate Chat Completions API calls with function execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391cabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate our get_articles and read_article_and_summarize functions\n",
    "arxiv_functions = [\n",
    "    {\n",
    "        \"name\": \"get_articles\",\n",
    "        \"description\": \"\"\"Use this function to get academic papers from arXiv to answer user questions.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            User query in JSON. Responses should be summarized and should include the article URL reference\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "        \"name\": \"read_article_and_summarize\",\n",
    "        \"description\": \"\"\"Use this function to read whole papers and provide a summary for users.\n",
    "        You should NEVER call this function before get_articles has been called in the conversation.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            Description of the article in plain text based on the user's query\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ccb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion_with_function_execution(messages, functions=[None]):\n",
    "    \"\"\"This function makes a ChatCompletion API call with the option of adding functions\"\"\"\n",
    "    response = chat_completion_request(messages, functions)\n",
    "    full_message = response.json()[\"choices\"][0]\n",
    "    if full_message[\"finish_reason\"] == \"function_call\":\n",
    "        print(f\"Function generation requested, calling function\")\n",
    "        return call_arxiv_function(messages, full_message)\n",
    "    else:\n",
    "        print(f\"Function not required, responding to user\")\n",
    "        return response.json()\n",
    "\n",
    "\n",
    "def call_arxiv_function(messages, full_message):\n",
    "    \"\"\"Function calling function which executes function calls when the model believes it is necessary.\n",
    "    Currently extended by adding clauses to this if statement.\"\"\"\n",
    "\n",
    "    if full_message[\"message\"][\"function_call\"][\"name\"] == \"get_articles\":\n",
    "        try:\n",
    "            parsed_output = json.loads(full_message[\"message\"][\"function_call\"][\"arguments\"])\n",
    "            print(\"Getting search results\")\n",
    "            results = get_articles(parsed_output[\"query\"])\n",
    "        except Exception as e:\n",
    "            print(parsed_output)\n",
    "            print(f\"Function execution failed\")\n",
    "            print(f\"Error message: {e}\")\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": full_message[\"message\"][\"function_call\"][\"name\"],\n",
    "                \"content\": str(results),\n",
    "            }\n",
    "        )\n",
    "        try:\n",
    "            print(\"Got search results, summarizing content\")\n",
    "            response = chat_completion_request(messages)\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            print(type(e))\n",
    "            raise Exception(\"Function chat request failed\")\n",
    "\n",
    "    elif full_message[\"message\"][\"function_call\"][\"name\"] == \"read_article_and_summarize\":\n",
    "        parsed_output = json.loads(full_message[\"message\"][\"function_call\"][\"arguments\"])\n",
    "        print(\"Finding and reading paper\")\n",
    "        summary = summarize_text(parsed_output[\"query\"])\n",
    "        return summary\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Function does not exist and cannot be called\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd3e7868",
   "metadata": {},
   "source": [
    "### arXiv conversation\n",
    "\n",
    "Let's test out our function in conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39a1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a system message\n",
    "paper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
    "You summarize the papers clearly so the customer can decide which to read to answer their question.\n",
    "You always provide the article_url and title so the user can understand the name of the paper and click through to access it.\n",
    "Begin!\"\"\"\n",
    "paper_conversation = Conversation()\n",
    "paper_conversation.add_message(\"system\", paper_system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253fd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a user message\n",
    "paper_conversation.add_message(\"user\", \"Hi, how does PPO reinforcement learning work?\")\n",
    "chat_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "assistant_message = chat_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "paper_conversation.add_message(\"assistant\", assistant_message)\n",
    "display(Markdown(assistant_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add another user message to induce our system to use the second tool\n",
    "paper_conversation.add_message(\n",
    "    \"user\",\n",
    "    \"Can you read the PPO sequence generation paper for me and give me a summary\",\n",
    ")\n",
    "updated_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "display(Markdown(updated_response[\"choices\"][0][\"message\"][\"content\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
